{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oej0HevG6_tC"},"outputs":[],"source":["!pip install --upgrade google-cloud-vision\n","!pip install utils"]},{"cell_type":"code","source":["import os\n","import io\n","import numpy as np\n","import pandas as pd\n","import platform\n","from PIL import ImageFont, ImageDraw, Image\n","import utils\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.cloud import vision\n","from google.cloud import vision_v1"],"metadata":{"id":"OmyGKBjp7Ccq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'ultra-airway-412008-69f1d43b4fb7.json'\n","\n","# client_options = {'api_endpoint': 'eu-vision.googleapis.com'}\n","# client = vision.ImageAnnotatorClient(client_options=client_options)\n","client_options = {'api_endpoint': 'vision.googleapis.com'}\n","client = vision.ImageAnnotatorClient(client_options=client_options)"],"metadata":{"id":"SqdIkXOj7Er6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('data2.csv',encoding = 'utf-8')\n","df=df.drop(columns = ['Unnamed: 0','level_0','index'])"],"metadata":{"id":"sxt6H3Ah7FjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_folder_path = '/content/im'\n","image_files = os.listdir(image_folder_path)"],"metadata":{"id":"3G2JAtxl7LdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ocr_with_google_cloud_vision(path):\n","    with open(image_path, 'rb') as image_file:\n","        content = image_file.read()\n","\n","    image = vision.Image(content=content)\n","\n","    response = client.text_detection(image=image)\n","    texts = response.text_annotations\n","    for text in texts:\n","        vertices = text.bounding_poly.vertices\n","        if len(vertices) == 4:\n","            x1, y1 = vertices[0].x, vertices[0].y\n","            x2, y2 = vertices[2].x, vertices[2].y\n","\n","            height = abs(y2 - y1)\n","            area = abs((x2 - x1) * (y2 - y1))\n","\n","            if height >= 47:\n","                return text.description.replace('\\n', ' ')\n","\n","    return ''"],"metadata":{"id":"_9VNMT-rMGDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 유해성 검출 함수\n","def detect_safe_search(path):\n","\n","\n","    with open(image_path, 'rb') as image_file:\n","        content = image_file.read()\n","\n","    image = vision.Image(content=content)\n","\n","\n","\n","    response = client.safe_search_detection(image=image)\n","    safe = response.safe_search_annotation\n","\n","    likelihood_name = (0,10,20,30,40,50)\n","\n","\n","    # score = likelihood_name[safe.adult]+likelihood_name[safe.medical]+likelihood_name[safe.spoof]+likelihood_name[safe.violence]+likelihood_name[safe.racy]\n","\n","\n","    if response.error.message:\n","        raise Exception(\n","            \"{}\\nFor more info on error messages, check: \"\n","            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n","        )\n","    return likelihood_name[safe.adult], likelihood_name[safe.medical], likelihood_name[safe.spoof],likelihood_name[safe.violence],likelihood_name[safe.racy]"],"metadata":{"id":"rBkJAEQW7VyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 제일 많은 색상의 RGB 비율 함수\n","def detect_properties(path):\n","    with open(image_path, 'rb') as image_file:\n","        content = image_file.read()\n","\n","    image = vision.Image(content=content)\n","\n","    response = client.image_properties(image=image)\n","    props = response.image_properties_annotation\n","    fraction = []\n","    max = 0\n","    cnt = 0\n","    for color in props.dominant_colors.colors:\n","        fraction.append([color.pixel_fraction,color.color.red,color.color.green,color.color.blue])\n","    for idx,i in enumerate(fraction):\n","        if max < fraction[idx][0]:\n","            max = fraction[idx][0]\n","            cnt = idx\n","    # print(fraction[cnt])\n","    # print(props)\n","    # print(fraction)\n","    # print(fraction[cnt])\n","\n","\n","\n","    if response.error.message:\n","        raise Exception(\n","            \"{}\\nFor more info on error messages, check: \"\n","            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n","        )\n","    return fraction[cnt]"],"metadata":{"id":"p23212257ehF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 썸네일 텍스트 위치 검출 함수\n","def process_image_with_text(path):\n","    with io.open(image_path, 'rb') as image_file:\n","        content = image_file.read()\n","    image = vision.Image(content=content)\n","    response = client.text_detection(image=image)\n","    texts = response.text_annotations\n","\n","    img = cv2.imread(path)\n","    roi_img = img.copy()\n","    height = img.shape[0]\n","    height1 = height * (1 / 3)\n","    height2 = height * (2 / 3)\n","    cnt = {\n","        'up': 0, 'middle': 0, 'down': 0,'none':0\n","    }\n","\n","    if not texts:\n","        cnt['none']+=1\n","    else:\n","        for text in texts[1:-1]:\n","\n","\n","            vertices = (['({},{})'.format(vertex.x, vertex.y)\n","                         for vertex in text.bounding_poly.vertices])\n","\n","            ocr_text = text.description\n","            y1 = text.bounding_poly.vertices[0].y\n","            y2 = text.bounding_poly.vertices[2].y\n","\n","            m_y = (abs(y2 + y1)) / 2\n","\n","            if 0 <= m_y and m_y < height1:\n","                cnt['up'] += len(ocr_text)\n","            elif height1 <= m_y and m_y < height2:\n","                cnt['middle'] += len(ocr_text)\n","            elif height2 <= m_y:\n","                cnt['down'] += len(ocr_text)\n","        if response.error.message:\n","            raise Exception(\n","                '{}\\nFor more info on error messages, check: '\n","                'https://cloud.google.com/apis/design/errors'.format(\n","                    response.error.message))\n","    max_key = max(cnt, key=cnt.get)\n","    return max_key\n","\n"],"metadata":{"id":"u9VakmgJ7jBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 얼굴로 사람수 인식 함수\n","def detect_faces_uri(path):\n","\n","\n","    client = vision.ImageAnnotatorClient()\n","\n","    # 이미지 파일을 읽어옵니다.\n","    with open(image_path, 'rb') as image_file:\n","        content = image_file.read()\n","\n","    # 이미지 데이터를 Vision API로 전송합니다.\n","    image = vision.Image(content=content)\n","\n","    response = client.face_detection(image=image)\n","    faces = response.face_annotations\n","\n","    if response.error.message:\n","        raise Exception(\n","            \"{}\\nFor more info on error messages, check: \"\n","            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n","        )\n","    return len(faces)"],"metadata":{"id":"0dL3aVz_7nYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 일정 높이 이상일때 텍스트가 썸네일을 차지하는 비율 함수\n","def calculate_char_heights_with_text(path):\n","    char_info = []\n","    with io.open(image_path, 'rb') as image_file:\n","        content = image_file.read()\n","    image = vision.Image(content=content)\n","    response = client.text_detection(image=image)\n","    texts = response.text_annotations\n","    for text in texts:\n","        vertices = text.bounding_poly.vertices\n","        if len(vertices) == 4:\n","            x1, y1 = vertices[0].x, vertices[0].y\n","            x2, y2 = vertices[2].x, vertices[2].y\n","\n","            height = abs(y2 - y1)\n","            area = abs((x2 - x1) * (y2 - y1))\n","\n","            if height >= 47:\n","                char_info.append(area)\n","\n","    char_info.remove(char_info[0])\n","    return char_info"],"metadata":{"id":"Duj2A6uZ7qzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지 파일 정렬함수\n","def sort_key(file_name):\n","    return int(file_name.split('_')[1].split('.')[0])\n","# image_files.remove('.ipynb_checkpoints')                  # 이미지 폴더 길이 확인해보고 이상한거 있으면 미리 제거\n","image_files_sorted = sorted(image_files, key=sort_key)"],"metadata":{"id":"jfWkkFHn7sr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, image_file in enumerate(image_files_sorted):\n","    image_path = os.path.join(image_folder_path, image_file)\n","\n","\n","    print(image_file)\n","    try:\n","        df.loc[idx, '썸네일 문자'] = ocr_with_google_cloud_vision(image_path)\n","\n","        x1, x2, x3, x4, x5 = detect_safe_search(image_path)\n","        df.loc[idx, 'Adult'] = x1\n","        df.loc[idx, 'Spoof'] = x2\n","        df.loc[idx, 'Medical'] = x3\n","        df.loc[idx, 'Violence'] = x4\n","        df.loc[idx,'Racy'] = x5\n","        properties = detect_properties(image_path)\n","        df.loc[idx, 'R'] = properties[1]\n","        df.loc[idx, 'G'] = properties[2]\n","        df.loc[idx, 'B'] = properties[3]\n","        df.loc[idx, '썸네일 텍스트 위치'] = process_image_with_text(image_path)\n","        df.loc[idx, '썸네일 사람수'] = detect_faces_uri(image_path)\n","        char_info_list = calculate_char_heights_with_text(image_path)\n","        total_area = sum(char_info for char_info in char_info_list[1:])\n","        portion = (total_area / 921600) * 100\n","        df.loc[idx, '썸네일 텍스트 비율'] = portion\n","        print(idx)\n","    except Exception as e:\n","        print(f\"Error processing image {image_path} (idx: {idx}): {str(e)}\")\n","        continue\n"],"metadata":{"id":"arPNbywF75x-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"HYy2ZyJM77MT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('',encoding=\"utf-8-sig\")"],"metadata":{"id":"lMlDo3KY780X"},"execution_count":null,"outputs":[]}]}